{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd0a133",
   "metadata": {},
   "source": [
    "# IBTrACS\n",
    "\n",
    "This tutorial explores the International Best Track Archive for Climate Stewardship (IBTrACS) dataset, which provides global tropical cyclone data. The dataset is available in multiple formats, including CSV, shapefiles, and NetCDF, which we will explore in this notebook.\n",
    "\n",
    "We will cover the following:\n",
    "- How to load and analyze IBTrACS data in CSV format.\n",
    "- How to work with shapefiles to visualize cyclone paths.\n",
    "- How to leverage the NetCDF format for more advanced analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a034d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd181fb9",
   "metadata": {},
   "source": [
    "## CSV Data\n",
    "\n",
    "In this section, we will load IBTrACS data in CSV format. The CSV format is straightforward and ideal for those who are familiar with tabular data. We will explore basic operations, such as loading the data into a DataFrame, filtering specific columns, and performing simple analyses.\n",
    "\n",
    "### Steps:\n",
    "1. Load the CSV file using Pandas.\n",
    "2. Inspect the data to understand its structure and content.\n",
    "3. Filter and analyze specific columns, such as storm names, dates, and locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#point to IBTrACS URL\n",
    "url = \"https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r01/access/csv/ibtracs.ALL.list.v04r01.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40837bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the IBTrACS CSV data into a DataFrame and display the first few rows to get an overview of the data.\n",
    "ibtracs_data = pd.read_csv(url) \n",
    "\n",
    "ibtracs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b7b81-c0bd-4861-a21a-4fd0b6da7f33",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Before diving deeper, let's explore the structure of the dataset. We’ll start by listing all the column names to understand what data is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2982c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of columns to a list\n",
    "column_list = ibtracs_data.columns.tolist()\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1848ea40-47ca-4b57-8844-5f6d3e6285ca",
   "metadata": {},
   "source": [
    "## Filtering the Dataset\n",
    "In this section, we’ll explore how to filter the dataset based on different criteria, such as a specific season or storm. This can be useful when you want to analyze a subset of the data.\n",
    "\n",
    "### Filtering by Season\n",
    "Let’s start by filtering the dataset to include only storms from the 2005 season.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d161adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Often we want to filter this dataset by different criteria. Here we do it by\n",
    "ibtracs_2005=ibtracs_data[ibtracs_data['SEASON'] == 2005]\n",
    "ibtracs_2005.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c16e564-d823-420f-82e1-a6a6bfdc7705",
   "metadata": {},
   "source": [
    "### Filtering by Storm Name or SID\n",
    "Next, we’ll filter the data to retrieve information about a specific storm. Here, we use the storm name \"PHOEBE.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f1406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data for a specific storm.  You can use either the name or the SID.  \n",
    "ibtracs_phoebe=ibtracs_data[ibtracs_data['NAME'] == 'PHOEBE']\n",
    "ibtracs_phoebe.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd3759-d114-48c9-93a1-c2a6d8151ca4",
   "metadata": {},
   "source": [
    "### Subsetting Data for Specific Variables\n",
    "Finally, we’ll create a subset of the data that includes only the latitude (LAT), longitude (LON), and storm speed (STORM_SPEED) for the storm \"PHOEBE.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aeda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset data on LAT, LON, and Storm Speed for Phoebe\n",
    "phoebe_subset = ibtracs_phoebe[['LAT', 'LON', 'STORM_SPEED']]\n",
    "phoebe_subset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6dc303-d50f-4874-9bfa-7cf91f4c7be8",
   "metadata": {},
   "source": [
    "## Wrapping Up the CSV Data Exploration\n",
    "\n",
    "In this section, we explored how to filter and subset the IBTrACS dataset using basic criteria like season and storm name. These techniques allow you to narrow down your analysis to specific storms or time periods, making it easier to extract meaningful insights from the data.\n",
    "\n",
    "### Moving Forward: Integrating Spatial Data with Shapefiles\n",
    "\n",
    "Next, we'll dive into working with shapefiles, a common format for spatial data in geographic information systems (GIS). Shapefiles are essential when you want to visualize storm tracks on a map or analyze how storms interact with specific geographic regions. Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622afac",
   "metadata": {},
   "source": [
    "## Shapefiles\n",
    "\n",
    "Shapefiles are a common format for geographic data. In this section, we will use Geopandas to load and analyze shapefiles containing tropical cyclone paths. This allows us to visualize cyclone tracks on a map, providing a spatial understanding of the data. Shape files consist of geometric features like points, lines, and polygons that represent real-world objects such as cities, roads, or boundaries.\n",
    "\n",
    "**Why Use Shapefiles?**\n",
    "\n",
    "Shapefiles allow you to visualize and analyze geographic data spatially. For instance, in the context of tropical cyclones, they can represent the paths of storms, enabling you to:\n",
    "\n",
    "- **Visualize Data:** See the spatial distribution of cyclone tracks on a map.\n",
    "- **Perform Spatial Analysis:** Analyze relationships between storm paths and other geographic features, like coastlines or population centers.\n",
    "- **Integrate with Other Data:** Overlay cyclone paths with other maps (e.g., population, infrastructure) for more comprehensive analysis.\n",
    "\n",
    "In this section, we’ll use Geopandas to load cyclone path shapefiles and visualize them on a map. Check the readme in this tutorial for notes on installing geopandas\n",
    "\n",
    "### Steps:\n",
    "1. Load the shapefiles using Geopandas.\n",
    "2. Plot the cyclone tracks on a map to visualize their paths.\n",
    "3. Explore the relationship between storm intensity and geography.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980f236-a551-443d-93ab-5110efe7753e",
   "metadata": {},
   "source": [
    "### Downloading Shapefile Data\n",
    "\n",
    "In this step, we download the zipped shapefile containing the cyclone tracks from an online source. We will use the `requests` library to handle the HTTP request. The shapefile is provided by the NOAA's International Best Track Archive for Climate Stewardship (IBTrACS).\n",
    "\n",
    "Downloading files programmatically can be particularly useful when working with large datasets or when you need to automate data retrieval. Here, we demonstrate how to download the shapefile and handle potential errors gracefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a07b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the zipped shapefile\n",
    "url = 'https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r01/access/shapefile/IBTrACS.ALL.list.v04r01.lines.zip'\n",
    "\n",
    "# Try to download the file and handle potential errors\n",
    "try:\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()  # This will raise an HTTPError if the request was unsuccessful\n",
    "    zip_file_content = response.content\n",
    "    print(\"File downloaded successfully.\")\n",
    "    \n",
    "    # Check the content's metadata\n",
    "    print(response.headers)\n",
    "    \n",
    "except requests.exceptions.HTTPError as err:\n",
    "    print(f\"HTTP error occurred: {err}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db485c6b-0849-470e-a6cf-a3c0ce6f6e4e",
   "metadata": {},
   "source": [
    "### Checking the Downloaded File's Metadata\n",
    "\n",
    "After successfully downloading the shapefile, it's important to verify what we have received. The `response.headers` attribute provides metadata about the downloaded file, such as its content type, size, and other HTTP headers. This information can be useful to confirm that the download was successful and that the file is in the expected format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae86e8d-8a51-448e-8a49-0c7c74b2ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the content's metadata\n",
    "if zip_file_content:\n",
    "    print(response.headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507192bb-1522-469c-8e4a-8d186c84f792",
   "metadata": {},
   "source": [
    "### Understanding the Response Headers\n",
    "\n",
    "After successfully downloading the shapefile, we can inspect the response headers to gain some insight into the file and the server's response. Here are a few key elements from the headers:\n",
    "\n",
    "- **Date:** The timestamp when the server processed the request (`'Wed, 14 Aug 2024 19:47:32 GMT'`).\n",
    "- **Content-Length:** The size of the downloaded file in bytes (`'46718951'`), which helps us understand how large the download is.\n",
    "- **Content-Type:** Indicates the type of file being returned, in this case, a ZIP archive (`'application/zip'`).\n",
    "- **Last-Modified:** The date and time when the file was last modified on the server (`'Tue, 13 Aug 2024 10:16:49 GMT'`).\n",
    "- **ETag:** A unique identifier for the file's version (`'\"2c8dfe7-61f8de85a9be0\"'`), which can be useful for caching.\n",
    "\n",
    "These headers provide important information about the file and the request, helping us confirm that we've received the correct content and understanding the file's characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffdc388-64a2-4360-8378-ee11ef95e81a",
   "metadata": {},
   "source": [
    "### Saving the Shapefile Locally\n",
    "\n",
    "After downloading the shapefile from the URL, the next step is to save it locally. This allows us to inspect the file manually and use it in further analysis. By saving the file, you can:\n",
    "\n",
    "1. **Ensure File Integrity:** Saving the file locally allows you to verify that the download was successful and the file is complete.\n",
    "2. **Manual Inspection:** You can open the file in a file explorer or a specialized GIS software to check its contents and structure.\n",
    "3. **Reuse in Future Work:** Keeping a local copy means you can reuse the data without needing to re-download it each time.\n",
    "\n",
    "The code below saves the downloaded content as a ZIP file named `IBTrACS.zip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77faa347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file locally for inspection\n",
    "with open('IBTrACS.zip', 'wb') as f:\n",
    "    f.write(zip_file_content)\n",
    "\n",
    "print(\"File saved locally as IBTrACS.zip. Inspect this file manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8eb30f-b773-4fd4-8124-e969a44470dd",
   "metadata": {},
   "source": [
    "### Extracting and Inspecting the Shapefile\n",
    "\n",
    "Now that we have the ZIP file saved locally, the next step is to extract its contents and locate the shapefile within. Shapefiles typically consist of several files, and we need to extract all of them to work with the geographic data.\n",
    "\n",
    "In this section, we will:\n",
    "1. **Inspect the Contents:** Look inside the ZIP file to see what files are included.\n",
    "2. **Extract Files:** Unzip the contents to a directory for further use.\n",
    "3. **Load the Shapefile:** Use Geopandas to load the shapefile and inspect the first few rows of data.\n",
    "\n",
    "Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('IBTrACS.zip', 'r') as z:\n",
    "    # Inspect contents to find the shapefile\n",
    "    print(z.namelist())  # This will show all files in the zip\n",
    "    # Extract the shapefile and associated files into a temporary directory\n",
    "    z.extractall('IBTrACS_unzipped')\n",
    "\n",
    "# Read the shapefile from the extracted files\n",
    "shapefile_path = 'IBTrACS_unzipped/IBTrACS.ALL.list.v04r01.lines.shp'\n",
    "ibtracs_gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Inspect the data\n",
    "print(ibtracs_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b62a352-c38b-40b2-a038-e4dae22b812f",
   "metadata": {},
   "source": [
    "### Filtering the Shapefile Data for a Specific Storm\n",
    "\n",
    "Now that we've successfully loaded the shapefile into a GeoDataFrame, we can start working with the data. In this step, we'll filter the data to focus on a specific storm. For demonstration purposes, we'll filter the data for the storm named \"PHOEBE.\"\n",
    "\n",
    "Filtering the data allows us to isolate and analyze the paths, intensities, and other characteristics of a particular storm, which can be very useful for targeted studies or visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3243cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'StormName' with the actual storm name you want to filter by\n",
    "phoebe_gdf = ibtracs_gdf[ibtracs_gdf['NAME'] == 'PHOEBE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201837e6-ed3c-4dab-8c01-0d51e572704a",
   "metadata": {},
   "source": [
    "### Creating a Subset of the GeoDataFrame\n",
    "\n",
    "After filtering the GeoDataFrame to isolate data for the storm \"PHOEBE,\" the next step is to create a subset of the data. Here, we'll focus on the geometry, latitude (LAT), and longitude (LON) columns. This subset will allow us to work more efficiently with just the essential information needed for mapping and analysis.\n",
    "\n",
    "By reducing the dataset to these key columns, we can streamline our subsequent analysis and visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the GeoDataFrame\n",
    "subsetphoebe_gdf = phoebe_gdf[['geometry', 'LAT', 'LON']]\n",
    "\n",
    "# Inspect the subset data\n",
    "print(subsetphoebe_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43842f-8968-4a8b-98a4-fd6e59d29510",
   "metadata": {},
   "source": [
    "### Plotting the Subset Data\n",
    "\n",
    "To begin visualizing the data, we will create a basic plot of the subsetted GeoDataFrame. While this initial plot, which simply marks the locations of the storm's path, may not be very informative on its own, it sets the stage for more detailed and meaningful visualizations using geographic maps.\n",
    "\n",
    "By plotting this data, we can quickly check that our subset was created correctly and that the storm's path data is being accurately represented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb983419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data (not that useful) \n",
    "subsetphoebe_gdf.plot(marker='o', color='blue', markersize=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca74f822-ce8e-435a-82cb-01afc4b181de",
   "metadata": {},
   "source": [
    "### Visualizing the Storm Track on a Map\n",
    "\n",
    "After obtaining and visualizing a basic plot of the storm's path, we can now enhance our visualization by plotting the data on an interactive map using Folium. This map will allow us to see the storm's track more clearly and interact with the data points.\n",
    "\n",
    "The map will include:\n",
    "- **Circle markers** to represent the positions of the storm along its path.\n",
    "- **A polyline** to connect these points, showing the storm's track over time.\n",
    "\n",
    "This visualization provides a more intuitive understanding of the storm's movement across geographic locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d6dd3-021d-4c21-b8b4-a04fc7434566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base map centered around the average latitude and longitude of the storm's path\n",
    "m = folium.Map(location=[subsetphoebe_gdf['LAT'].mean(), subsetphoebe_gdf['LON'].mean()], zoom_start=5)\n",
    "\n",
    "# Add storm track to the map\n",
    "for _, row in subsetphoebe_gdf.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['LAT'], row['LON']],\n",
    "        radius=5,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add a polyline to connect the storm track points\n",
    "coordinates = subsetphoebe_gdf[['LAT', 'LON']].values.tolist()\n",
    "folium.PolyLine(locations=coordinates, color='blue').add_to(m)\n",
    "\n",
    "# Display the map in the notebook\n",
    "m\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918851e",
   "metadata": {},
   "source": [
    "\n",
    "## NetCDF\n",
    "\n",
    "The NetCDF format is highly efficient for handling large and complex datasets, especially those with multiple dimensions like time and space. In this section, we will load and analyze the IBTrACS data in NetCDF format. This allows us to perform more advanced analyses, such as tracking storm progression over time.\n",
    "\n",
    "### Steps:\n",
    "1. Load the NetCDF file using the `netCDF4` library.\n",
    "2. Extract specific variables like latitude, longitude, and wind speed.\n",
    "3. Plot the time series data to analyze the progression of a specific storm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe5124e-cb98-4bd8-99fa-e462d1f76cfb",
   "metadata": {},
   "source": [
    "### Transitioning from Shapefiles to NetCDF4\n",
    "\n",
    "While shapefiles gave us a powerful way to visualize tropical cyclone tracks spatially, NetCDF4 files provide a more detailed and complex representation of storm data across multiple dimensions. This is particularly useful when dealing with large-scale, time-series data, where we want to analyze not just where storms occurred but also how their characteristics evolved over time.\n",
    "\n",
    "Let’s start by loading the NetCDF4 data and examining its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a959837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ee4844-436a-4412-b407-b71226856c4d",
   "metadata": {},
   "source": [
    "## Accessing IBTrACS Data in NetCDF Format\n",
    "\n",
    "This data is  maintained by the NOAA National Centers for Environmental Information (NCEI).\n",
    "\n",
    "The dataset we are using is in NetCDF format, which is ideal for storing large, multi-dimensional data such as the spatial and temporal information of tropical cyclones. NetCDF files can contain multiple variables and dimensions, making them well-suited for complex atmospheric and oceanographic datasets.\n",
    "\n",
    "You can explore more about the IBTrACS dataset and access additional data formats [here](https://www.ncei.noaa.gov/products/international-best-track-archive).\n",
    "\n",
    "### Loading the NetCDF Data\n",
    "\n",
    "Let's start by accessing the NetCDF file from the IBTrACS archive:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02700c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# URL to the NetCDF file\n",
    "url = \"https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r01/access/netcdf/IBTrACS.ALL.v04r01.nc\" \n",
    "\n",
    "# Explanation:\n",
    "# The above URL points to the latest version (v04r01) of the IBTrACS dataset in NetCDF format.\n",
    "# This file contains global tropical cyclone data, ,including information on storm tracks, intensity, and other key features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7490673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open('IBTrACS.ALL.v04r01.nc', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"File downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a5b069-e7c7-47e7-8dee-dd920224f276",
   "metadata": {},
   "source": [
    "### Understanding the NetCDF4 Structure\n",
    "\n",
    "The output from the code above provides a detailed overview of the NetCDF4 file, including its dimensions, variables, and attributes. Each variable in the file represents a different type of data, such as latitude, longitude, time, or specific meteorological measurements. Understanding this structure is crucial for selecting and extracting the right data for your analysis.\n",
    "\n",
    "Next, we will extract specific variables related to storm intensity, location, and time, and visualize them to see how the storm's characteristics evolve over its lifetime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the NetCDF file\n",
    "nc_file = Dataset('IBTrACS.ALL.v04r01.nc', mode='r')\n",
    "\n",
    "# Inspect the file structure\n",
    "print(nc_file)\n",
    "\n",
    "# List all variables in the file\n",
    "print(nc_file.variables.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4dbb8-19bf-4239-a637-b66b07befea6",
   "metadata": {},
   "source": [
    "\n",
    "### Extracting Latitude and Longitude\n",
    "\n",
    "First, we will access the latitude and longitude variables from the NetCDF file to understand the geographical spread of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access specific variables\n",
    "latitudes = nc_file.variables['lat'][:]\n",
    "longitudes = nc_file.variables['lon'][:]\n",
    "\n",
    "# Inspect the data\n",
    "print(latitudes)\n",
    "print(longitudes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa5a60-12c7-4942-88c1-85d62dcd2560",
   "metadata": {},
   "source": [
    "### Filtering Data for a Specific Storm\n",
    "\n",
    "Next, we'll extract additional variables, including storm IDs, wind speed, pressure, and time. We'll filter the dataset for a specific storm ID (SID) to visualize its track. We'll also handle missing data and visualize the storm's path on a map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda8cf4-ed3a-4030-8c8c-0b9c3b17cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variables in the dataset:\")\n",
    "print(nc_file.variables.keys())\n",
    "\n",
    "# Print details of each variable\n",
    "for var in nc_file.variables:\n",
    "    print(f\"Variable: {var}\")\n",
    "    print(nc_file.variables[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e30ec2-87da-442a-8034-9b168ac25888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the relevant variables from the NetCDF file\n",
    "storm_ids = nc_file.variables['sid'][:]\n",
    "latitudes = nc_file.variables['lat'][:]\n",
    "longitudes = nc_file.variables['lon'][:]\n",
    "wmo_wind = nc_file.variables['wmo_wind'][:]\n",
    "wmo_pres = nc_file.variables['wmo_pres'][:]\n",
    "times = nc_file.variables['time'][:]\n",
    "\n",
    "# Convert storm IDs to strings\n",
    "storm_ids = [''.join(sid.astype(str)).strip() for sid in storm_ids]\n",
    "\n",
    "# Identify unique storm IDs in the dataset\n",
    "unique_storm_ids = set(storm_ids)\n",
    "print(f\"Unique Storm IDs: {list(unique_storm_ids)[:10]}\")  # Display the first 10 unique storm IDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424495e5-e3ed-49a7-a9d6-287ae732b015",
   "metadata": {},
   "source": [
    "### Selecting and Verifying a Specific Storm\n",
    "\n",
    "Next, we select a specific storm ID for further analysis and verify if it is present in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e69ba-a5a0-444c-919a-d6cbb3702d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target a specific storm ID for analysis\n",
    "target_sid = '2012166N09269'  # Example: using the first unique storm ID\n",
    "print(f\"Target SID: {target_sid}\")\n",
    "\n",
    "# Verify if the target SID is present in the dataset\n",
    "if target_sid in storm_ids:\n",
    "    print(f\"Storm ID {target_sid} found in the dataset.\")\n",
    "else:\n",
    "    print(f\"Storm ID {target_sid} not found in the dataset.\")\n",
    "    exit()  # Exit if the target SID is not found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1bd2b1-9fc8-4d5e-a0f9-091ecae0f750",
   "metadata": {},
   "source": [
    "### Handling Missing Data and Extracting Storm Data\n",
    "\n",
    "We then check for any missing data in the latitudes and longitudes and extract the relevant data points for the selected storm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c5413f-d501-4913-9154-acdbf00a4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data in latitudes and longitudes\n",
    "print(f\"NaN in latitudes: {np.isnan(latitudes).any()}\")\n",
    "print(f\"NaN in longitudes: {np.isnan(longitudes).any()}\")\n",
    "\n",
    "# Identify the indices corresponding to the target storm ID\n",
    "storm_indices = np.where(np.array(storm_ids) == target_sid)[0]\n",
    "print(f\"Storm indices for {target_sid}: {storm_indices}\")\n",
    "\n",
    "# Extract relevant data for the selected storm\n",
    "filtered_lats = latitudes[storm_indices].flatten()\n",
    "filtered_lons = longitudes[storm_indices].flatten()\n",
    "filtered_winds = wmo_wind[storm_indices].flatten()\n",
    "filtered_pressures = wmo_pres[storm_indices].flatten()\n",
    "filtered_times = times[storm_indices].flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a5f314-dd49-44cd-aad3-84aadef6f274",
   "metadata": {},
   "source": [
    "### Filtering and Visualizing the Storm Track\n",
    "\n",
    "Finally, we filter the data to remove any invalid points and visualize the storm's path on a map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c206129-f1eb-4189-a93f-76c80bbefbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter valid data points, removing any with NaN values or invalid measurements\n",
    "valid_data = [(lat, lon, wind, pres, t) for lat, lon, wind, pres, t in zip(filtered_lats, filtered_lons, filtered_winds, filtered_pressures, filtered_times) if not np.isnan(lat) and not np.isnan(lon) and wind != -9999 and pres != -9999]\n",
    "\n",
    "if valid_data:\n",
    "    filtered_lats, filtered_lons, filtered_winds, filtered_pressures, filtered_times = zip(*valid_data)\n",
    "\n",
    "    # Create a base map centered on the mean latitude and longitude\n",
    "    m = folium.Map(location=[np.mean(filtered_lats), np.mean(filtered_lons)], zoom_start=5)\n",
    "\n",
    "    # Add the storm track to the map\n",
    "    for lat, lon in zip(filtered_lats, filtered_lons):\n",
    "        folium.CircleMarker(\n",
    "            location=[lat, lon],\n",
    "            radius=5,\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fill_color='blue',\n",
    "            fill_opacity=0.6\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Add a polyline to connect the storm's path\n",
    "    coordinates = list(zip(filtered_lats, filtered_lons))\n",
    "    folium.PolyLine(locations=coordinates, color='blue').add_to(m)\n",
    "\n",
    "    # Display the map in the notebook\n",
    "    display(m)\n",
    "else:\n",
    "    print(\"No valid data points to plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606f9f0-2586-4f77-b3e5-2f52e696d101",
   "metadata": {},
   "source": [
    "## Visualizing Storm Data Over Time: Wind Speed and Pressure\n",
    "\n",
    "In this section, we'll take our analysis a step further by visualizing how key storm metrics—specifically wind speed and pressure—change over time. Understanding these time-based dynamics is crucial for interpreting the storm's behavior and its potential impacts.\n",
    "\n",
    "We'll start by converting the time data, which is stored in Julian days, into standard calendar dates. This will make our time series easier to understand and interpret. After that, we'll create a dual-axis plot to simultaneously visualize the changes in wind speed and pressure as the storm progresses. This plot will help us observe any correlations or patterns between these two variables during the storm's lifecycle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b446844-a939-4dcf-a4a0-2adbd88d777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Convert Julian days to calendar dates\n",
    "start_date = datetime(1858, 11, 17)  # Julian day zero reference\n",
    "filtered_dates = [start_date + timedelta(days=t) for t in filtered_times]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a438576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the figure and axis objects\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the wind speed on the left y-axis\n",
    "ax1.plot(filtered_dates, filtered_winds, color='blue')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Wind Speed (kts)', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Create a second y-axis to plot the pressure\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(filtered_dates, filtered_pressures, color='red')\n",
    "ax2.set_ylabel('Pressure (mb)', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Improve layout and add a title\n",
    "plt.title(f'Time Series for Storm {target_sid}', fontsize=14)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
